{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Ender's Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will upload some books from the Ender's game universe, process them using *nltk*, and embed the words in the corpus using *word2vec*.\n",
    "\n",
    "This is inspired by [this video](https://youtu.be/pY9EwZ02sXU) and implemented using [this source code](https://github.com/llSourcell/word_vectors_game_of_thrones-LIVE).\n",
    "\n",
    "**Thank you Siraj Raval!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#future is the missing compatibility layer between Python 2 and Python 3. \n",
    "#It allows you to use a single, clean Python 3.x-compatible codebase to \n",
    "#support both Python 2 and Python 3 with minimal overhead.\n",
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#encoding. word encodig\n",
    "import codecs\n",
    "#finds all pathnames matching a pattern, like regex\n",
    "import glob\n",
    "#log events for libraries\n",
    "import logging\n",
    "#concurrency\n",
    "import multiprocessing\n",
    "#dealing with operating system , like reading file\n",
    "import os\n",
    "#pretty print, human readable\n",
    "import pprint\n",
    "#regular expressions\n",
    "import re\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#natural language toolkit\n",
    "import nltk\n",
    "#dimensionality reduction\n",
    "import sklearn.manifold\n",
    "from bhtsne import tsne\n",
    "#math\n",
    "import numpy as np\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "#parse dataset\n",
    "import pandas as pd\n",
    "#visualization\n",
    "import seaborn as sns\n",
    "#word 2 vec\n",
    "from gensim.models import word2vec as w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess using *nltk*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/baralya/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/baralya/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stopwords like the at a an, unnecesasry\n",
    "#tokenization into sentences, punkt \n",
    "#http://www.nltk.org/\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialize rawunicode , all text goes here\n",
    "corpus_raw = u\"\"\n",
    "with codecs.open('Ender.txt', \"r\", \"utf-8\") as book_file:\n",
    "        corpus_raw += book_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" ENDER'S G\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_raw[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenizastion! saved the trained model here\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenize into sentences\n",
    "raw_sentences = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ENDER\\'S GAME\\n by Orson Scott Card\\n Chapter 1 -- Third\\n\\n \"I\\'ve watched through his eyes, I\\'ve listened through his ears, and tell you he\\'s the one.',\n 'Or at least as close as we\\'re going to get.\"',\n '\"That\\'s what you said about the brother.\"',\n '\"The brother tested out impossible.',\n 'For other reasons.',\n 'Nothing to do with his ability.\"',\n '\"Same with the sister.',\n 'And there are doubts about him.',\n \"He's too malleable.\",\n 'Too willing\\nto submerge himself in someone else\\'s will.\"']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_sentences[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert into list of words\n",
    "#remove unecessary characters, split into words, no hyhens and shit\n",
    "#split into words\n",
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for each sentece, sentences where each word is tokenized\n",
    "sentences = []\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ENDER',\n  'S',\n  'GAME',\n  'by',\n  'Orson',\n  'Scott',\n  'Card',\n  'Chapter',\n  'Third',\n  'I',\n  've',\n  'watched',\n  'through',\n  'his',\n  'eyes',\n  'I',\n  've',\n  'listened',\n  'through',\n  'his',\n  'ears',\n  'and',\n  'tell',\n  'you',\n  'he',\n  's',\n  'the',\n  'one'],\n ['Or', 'at', 'least', 'as', 'close', 'as', 'we', 're', 'going', 'to', 'get'],\n ['That', 's', 'what', 'you', 'said', 'about', 'the', 'brother'],\n ['The', 'brother', 'tested', 'out', 'impossible'],\n ['For', 'other', 'reasons'],\n ['Nothing', 'to', 'do', 'with', 'his', 'ability'],\n ['Same', 'with', 'the', 'sister'],\n ['And', 'there', 'are', 'doubts', 'about', 'him'],\n ['He', 's', 'too', 'malleable'],\n ['Too',\n  'willing',\n  'to',\n  'submerge',\n  'himself',\n  'in',\n  'someone',\n  'else',\n  's',\n  'will']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing to do with his ability.\"\n['Nothing', 'to', 'do', 'with', 'his', 'ability']\n"
     ]
    }
   ],
   "source": [
    "#print an example\n",
    "print(raw_sentences[5])\n",
    "print(sentence_to_wordlist(raw_sentences[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book corpus contains 503,902 tokens\n"
     ]
    }
   ],
   "source": [
    "#count tokens, each one being a sentence\n",
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print(\"The book corpus contains {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#step 2 build our model, another one is Glove\n",
    "#define hyperparameters\n",
    "\n",
    "# Dimensionality of the resulting word vectors.\n",
    "#more dimensions mean more traiig them, but more generalized\n",
    "num_features = 300\n",
    "\n",
    "#\n",
    "# Minimum word count threshold.\n",
    "min_word_count = 3\n",
    "\n",
    "# Number of threads to run in parallel.\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "# Context window length.\n",
    "context_size = 7\n",
    "\n",
    "# Downsample setting for frequent words.\n",
    "#rate 0 and 1e-5 \n",
    "#how often to use\n",
    "downsampling = 1e-3\n",
    "\n",
    "# Seed for the RNG, to make the results reproducible.\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ender2vec = w2v.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:21,796 : INFO : collecting all words and their counts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:21,797 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:21,828 : INFO : PROGRESS: at sentence #10000, processed 111908 words, keeping 8059 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:21,861 : INFO : PROGRESS: at sentence #20000, processed 228097 words, keeping 11485 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:21,891 : INFO : PROGRESS: at sentence #30000, processed 353550 words, keeping 14782 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:21,925 : INFO : PROGRESS: at sentence #40000, processed 478152 words, keeping 17718 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:21,931 : INFO : collected 18299 word types from a corpus of 503902 raw words and 42110 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:21,932 : INFO : Loading a fresh vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:21,962 : INFO : min_count=3 retains 7725 unique words (42% of original 18299, drops 10574)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:21,962 : INFO : min_count=3 leaves 490596 word corpus (97% of original 503902, drops 13306)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:21,989 : INFO : deleting the raw counts dictionary of 18299 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:21,991 : INFO : sample=0.001 downsamples 66 most-common words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:21,992 : INFO : downsampling leaves estimated 369461 word corpus (75.3% of prior 490596)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:21,992 : INFO : estimated required memory for 7725 words and 300 dimensions: 22402500 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:22,040 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "ender2vec.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec vocabulary length: 7725\n"
     ]
    }
   ],
   "source": [
    "print(\"Word2Vec vocabulary length:\", len(ender2vec.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:30,404 : INFO : training model with 4 workers on 7725 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:31,456 : INFO : PROGRESS: at 17.04% examples, 301680 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:32,470 : INFO : PROGRESS: at 35.81% examples, 320435 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:33,475 : INFO : PROGRESS: at 54.50% examples, 327694 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:34,480 : INFO : PROGRESS: at 72.84% examples, 329649 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:35,496 : INFO : PROGRESS: at 91.43% examples, 331517 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:35,940 : INFO : worker thread finished; awaiting finish of 3 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:35,952 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:35,988 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:35,990 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:35,990 : INFO : training on 2519510 raw words (1848269 effective words) took 5.6s, 331264 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1848269"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train model on sentneces\n",
    "ender2vec.train(sentences, \n",
    "        total_examples=ender2vec.corpus_count, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's it! we embedded our word vectors!\n",
    "\n",
    "### Now Let's play..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "if not os.path.exists(\"trained\"):\n",
    "    os.makedirs(\"trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:54,820 : INFO : saving Word2Vec object under trained/ender2vec.w2v, separately None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:54,821 : INFO : not storing attribute syn0norm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:54,821 : INFO : not storing attribute cum_table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-05 20:22:54,970 : INFO : saved trained/ender2vec.w2v\n"
     ]
    }
   ],
   "source": [
    "ender2vec.save(os.path.join(\"trained\", \"ender2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load model\n",
    "#ender2vec = w2v.Word2Vec.load(os.path.join(\"trained\", \"ender2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform dimentionality reduction using *tsne* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#put it all into a giant matrix\n",
    "all_word_vectors_matrix = ender2vec.wv.syn0\n",
    "#train t sne\n",
    "all_word_vectors_matrix_2d = tsne(all_word_vectors_matrix.astype(\n",
    "        'float64'))\n",
    "\n",
    "\n",
    "\n",
    "#squash dimensionality to 2\n",
    "#https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm\n",
    "#tsne = sklearn.manifold.TSNE(n_components=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot point in 2d space\n",
    "points = pd.DataFrame(\n",
    "    [\n",
    "        (word, coords[0], coords[1])\n",
    "        for word, coords in [\n",
    "            (word, all_word_vectors_matrix_2d[ender2vec.wv.vocab[word].index])\n",
    "            for word in ender2vec.wv.vocab\n",
    "        ]\n",
    "    ],\n",
    "    columns=[\"word\", \"x\", \"y\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ribs</td>\n",
       "      <td>15.631265</td>\n",
       "      <td>-21.844936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jail</td>\n",
       "      <td>6.789067</td>\n",
       "      <td>2.182995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>port</td>\n",
       "      <td>2.274317</td>\n",
       "      <td>-13.356660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>main</td>\n",
       "      <td>12.277898</td>\n",
       "      <td>-30.154868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thanked</td>\n",
       "      <td>5.798963</td>\n",
       "      <td>3.484200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stiff</td>\n",
       "      <td>14.617314</td>\n",
       "      <td>-24.195186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>thrill</td>\n",
       "      <td>16.773382</td>\n",
       "      <td>-18.393301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>slot</td>\n",
       "      <td>-29.835675</td>\n",
       "      <td>7.878562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rotation</td>\n",
       "      <td>-8.903737</td>\n",
       "      <td>11.813079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ask</td>\n",
       "      <td>-34.672347</td>\n",
       "      <td>5.025660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ribs</td>\n",
       "      <td>15.631265</td>\n",
       "      <td>-21.844936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jail</td>\n",
       "      <td>6.789067</td>\n",
       "      <td>2.182995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>port</td>\n",
       "      <td>2.274317</td>\n",
       "      <td>-13.356660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>main</td>\n",
       "      <td>12.277898</td>\n",
       "      <td>-30.154868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thanked</td>\n",
       "      <td>5.798963</td>\n",
       "      <td>3.484200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stiff</td>\n",
       "      <td>14.617314</td>\n",
       "      <td>-24.195186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>thrill</td>\n",
       "      <td>16.773382</td>\n",
       "      <td>-18.393301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>slot</td>\n",
       "      <td>-29.835675</td>\n",
       "      <td>7.878562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rotation</td>\n",
       "      <td>-8.903737</td>\n",
       "      <td>11.813079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ask</td>\n",
       "      <td>-34.672347</td>\n",
       "      <td>5.025660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff6dccdf668>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.plot.scatter(\"x\", \"y\", s=10, figsize=(20, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_region(x_bounds, y_bounds):\n",
    "    slice = points[\n",
    "        (x_bounds[0] <= points.x) &\n",
    "        (points.x <= x_bounds[1]) & \n",
    "        (y_bounds[0] <= points.y) &\n",
    "        (points.y <= y_bounds[1])\n",
    "    ]\n",
    "    \n",
    "    ax = slice.plot.scatter(\"x\", \"y\", s=35, figsize=(8, 6))\n",
    "    for i, point in slice.iterrows():\n",
    "        ax.text(point.x + 0.005, point.y + 0.005, point.word, fontsize=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's have a closer look at some interesting regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_region(x_bounds=(22.0, 25.0), y_bounds=(12.0, 16.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_region(x_bounds=(4.0,6.0), y_bounds=(31.0, 35.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try some synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ender2vec.most_similar(\"Bean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ender2vec.most_similar(\"battleroom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ender2vec.most_similar(\"Buggers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at some word associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#distance, similarity, and ranking\n",
    "def nearest_similarity_cosmul(start1, end1, end2):\n",
    "    similarities = ender2vec.most_similar_cosmul(\n",
    "        positive=[end2, start1],\n",
    "        negative=[end1]\n",
    "    )\n",
    "    start2 = similarities[0][0]\n",
    "    print(\"{start1} is related to {end1}, as {start2} is related to {end2}\".format(**locals()))\n",
    "    return start2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nearest_similarity_cosmul(\"Ender\", \"Valentine\", \"Bean\")\n",
    "# interesting: http://enderverse.wikia.com/wiki/Suriyawong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nearest_similarity_cosmul(\"Peter\", \"Valentine\", \"Locke\")\n",
    "# amazing!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nearest_similarity_cosmul(\"run\", \"slow\", \"fight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nearest_similarity_cosmul(\"love\", \"hate\", \"formics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}